from __future__ import annotations

import os
from dataclasses import dataclass
from datetime import datetime, timezone
from typing import Dict, List, Optional, Tuple

from sqlalchemy import and_, select
from sqlalchemy.exc import SQLAlchemyError
from sqlalchemy.orm import Session

from services.pulsecast_api.app.core.logging import get_logger
from services.pulsecast_api.app.models.alerts import Alert
from services.pulsecast_api.app.models.forecasts import Forecast

logger = get_logger(__name__)

DEFAULT_THRESHOLD_PCT = 20.0
DEFAULT_MAX_ROWS = 1000

SEVERITY_MEDIUM = "MEDIUM"
SEVERITY_HIGH = "HIGH"
SEVERITY_CRITICAL = "CRITICAL"


@dataclass
class DriftRecord:
    sku_id: str
    year_month: str
    prev_p50: float
    latest_p50: float
    delta_pct: float
    severity: str


def _get_threshold() -> float:
    try:
        return float(os.getenv("ALERT_FORECAST_DRIFT_THRESHOLD_PCT", DEFAULT_THRESHOLD_PCT))
    except ValueError:
        logger.warning("Invalid ALERT_FORECAST_DRIFT_THRESHOLD_PCT; using default %s", DEFAULT_THRESHOLD_PCT)
        return DEFAULT_THRESHOLD_PCT


def _get_max_rows() -> int:
    try:
        return int(os.getenv("ALERT_MAX_ROWS_PER_RUN", DEFAULT_MAX_ROWS))
    except ValueError:
        logger.warning("Invalid ALERT_MAX_ROWS_PER_RUN; using default %s", DEFAULT_MAX_ROWS)
        return DEFAULT_MAX_ROWS


def get_latest_two_runs(db: Session) -> List[Tuple[str, datetime]]:
    stmt = (
        select(Forecast.run_id, Forecast.created_at)
        .distinct()
        .order_by(Forecast.created_at.desc())
        .limit(2)
    )
    rows = list(db.execute(stmt))
    return [(row[0], row[1]) for row in rows]


def load_forecasts_for_run(db: Session, run_id: str, max_rows: int) -> Dict[Tuple[str, str], float]:
    stmt = (
        select(Forecast.sku_id, Forecast.year_month, Forecast.p50)
        .where(Forecast.run_id == run_id)
        .limit(max_rows)
    )
    records: Dict[Tuple[str, str], float] = {}
    for sku_id, ym, p50 in db.execute(stmt):
        if ym is None:
            continue
        ym_str = ym.strftime("%Y-%m")
        records[(sku_id, ym_str)] = float(p50) if p50 is not None else 0.0
    return records


def _severity_from_delta_pct(delta_pct: float, threshold_pct: float) -> Optional[str]:
    abs_pct = abs(delta_pct) * 100.0
    if abs_pct < threshold_pct:
        return None
    if abs_pct >= 50.0:
        return SEVERITY_CRITICAL
    if abs_pct >= 30.0:
        return SEVERITY_HIGH
    return SEVERITY_MEDIUM


def compute_drift(
    prev_map: Dict[Tuple[str, str], float],
    latest_map: Dict[Tuple[str, str], float],
    threshold_pct: float,
) -> List[DriftRecord]:
    drifts: List[DriftRecord] = []
    keys = set(prev_map.keys()) & set(latest_map.keys())
    for key in keys:
        prev_p50 = prev_map.get(key, 0.0)
        latest_p50 = latest_map.get(key, 0.0)
        if prev_p50 <= 0:
            continue
        delta = latest_p50 - prev_p50
        delta_pct = delta / prev_p50
        severity = _severity_from_delta_pct(delta_pct, threshold_pct)
        if severity is None:
            continue
        drifts.append(
            DriftRecord(
                sku_id=key[0],
                year_month=key[1],
                prev_p50=prev_p50,
                latest_p50=latest_p50,
                delta_pct=delta_pct,
                severity=severity,
            )
        )
    return drifts


def insert_alerts_for_drift(
    db: Session,
    drifts: List[DriftRecord],
    latest_run_id: str,
    prev_run_id: str,
) -> int:
    if not drifts:
        logger.info("No drift above threshold; no alerts created.")
        return 0

    now = datetime.now(timezone.utc)
    alerts = []
    for d in drifts:
        direction = "+" if d.delta_pct >= 0 else ""
        percent_str = f"{direction}{d.delta_pct * 100:.1f}%"
        message = (
            f"Forecast P50 changed by {percent_str} for {d.sku_id} {d.year_month} "
            f"between runs {prev_run_id} -> {latest_run_id}"
        )
        alerts.append(
            Alert(
                alert_id=None,  # UUID generated by DB default if configured; otherwise handled by DB
                indicator_id=None,
                sku_id=d.sku_id,
                geo_id=None,
                alert_type="FORECAST_SHIFT",
                severity=d.severity,
                status="OPEN",
                message=message,
                triggered_at=now,
                acknowledged_at=None,
                created_at=now,
                updated_at=now,
            )
        )

    try:
        db.add_all(alerts)
        db.commit()
        return len(alerts)
    except SQLAlchemyError as exc:
        db.rollback()
        logger.error("Failed to insert drift alerts", exc_info=exc)
        raise


def run_early_warning(db: Session) -> None:
    threshold = _get_threshold()
    max_rows = _get_max_rows()
    logger.info("Running early warning with threshold=%s%% max_rows=%s", threshold, max_rows)

    runs = get_latest_two_runs(db)
    if len(runs) < 2:
        logger.warning("Fewer than two forecast runs; skipping early warning.")
        return

    latest_run_id, latest_created = runs[0]
    prev_run_id, prev_created = runs[1]
    logger.info("Using runs latest=%s (%s) prev=%s (%s)", latest_run_id, latest_created, prev_run_id, prev_created)

    try:
        latest_map = load_forecasts_for_run(db, latest_run_id, max_rows)
        prev_map = load_forecasts_for_run(db, prev_run_id, max_rows)
    except SQLAlchemyError as exc:
        logger.error("Failed to load forecasts for runs", exc_info=exc)
        raise

    logger.info("Comparing %s prev records and %s latest records", len(prev_map), len(latest_map))
    drifts = compute_drift(prev_map, latest_map, threshold)
    created = insert_alerts_for_drift(db, drifts, latest_run_id, prev_run_id)
    logger.info("Drift detection complete; alerts created: %s", created)
